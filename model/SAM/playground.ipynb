{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "<__main__.COCODataset object at 0x00000248B72837D0>\n",
      "shape of sample_img: torch.Size([3, 1024, 1024])\n",
      "shape fo sample_mask: torch.Size([178, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Huge credits: ng-sam-for-images-with-multiple-masks-34514ee811bb\n",
    "# Huggingface discussion: https://discuss.huggingface.co/t/fine-tuning-segment-anything-model-call-up-a-saved-model/57634\n",
    "# SAM: https://github.com/facebookresearch/segment-anything\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# dir\n",
    "TRAIN = os.path.join(\"./Dataset/Train\")\n",
    "TEST = os.path.join(\"./Dataset/Test\")\n",
    "ANNOT = \"annotations.json\"\n",
    "\n",
    "# training parameters\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1 # initial job setting\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# loss\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "EVALUATE=False\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import glob\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load data from a json file in COCO format.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        the root directory containing the images and annotations\n",
    "    annotation_file : str\n",
    "        name of the json file containing the annotations (in root_dir)\n",
    "    transform : callable\n",
    "        a function/transform to apply to each image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx)\n",
    "        returns the image, image path, and masks for the given index\n",
    "    buid_total_mask(masks)\n",
    "        combines the masks into a single mask\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        # Filter out image_ids without any annotations\n",
    "        self.image_ids = [image_id for image_id in self.image_ids if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return image, image_path, torch.tensor(masks).float()\n",
    "\n",
    "    def get_totalmask(self, masks):\n",
    "        \"\"\"get all masks in to one image\n",
    "        ARGS:\n",
    "            masks (List[Tensor]): list of masks\n",
    "        RETURNS:\n",
    "            total_gt (Tensor): all masks in one image\n",
    "\n",
    "        \"\"\"\n",
    "        total_gt = torch.zeros_like(masks[0][0,:,:])\n",
    "        for k in range(len(masks[0])):\n",
    "            total_gt += masks[0][k,:,:]\n",
    "        return total_gt\n",
    "\n",
    "class ResizeAndPad:\n",
    "    \"\"\"\n",
    "    Resize and pad images and masks to a target size.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_size : int\n",
    "        the target size of the image\n",
    "    transform : ResizeLongestSide\n",
    "        a transform to resize the image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # Resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "        image = self.transform.apply_image(image)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "\n",
    "        # Pad image and masks to form a square\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "\n",
    "        return image, masks, bboxes\n",
    "\n",
    "def load_datasets(img_size = 1024):\n",
    "    \"\"\" load the training and validation datasets in PyTorch DataLoader objects\n",
    "    ARGS:\n",
    "        img_size (int): image size\n",
    "    RETURNS:\n",
    "        train_dataloader (DataLoader): training dataset\n",
    "        val_dataloader (DataLoader): validation dataset\n",
    "\n",
    "    \"\"\"\n",
    "    transform = ResizeAndPad(img_size) # Fixed due to the limitation of SAM\n",
    "    traindata = COCODataset(root_dir=TRAIN,\n",
    "                        annotation_file=os.path.join(TRAIN, ANNOT),\n",
    "                        transform=transform)\n",
    "    valdata = COCODataset(root_dir=TEST,\n",
    "                      annotation_file=os.path.join(TEST, ANNOT),\n",
    "                      transform=transform)\n",
    "    train_dataloader = DataLoader(traindata,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=1)\n",
    "    val_dataloader = DataLoader(valdata,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "trainloader, validloader = load_datasets()\n",
    "sample_img = trainloader.dataset[0][0]\n",
    "sample_mask = trainloader.dataset[0][2]\n",
    "\n",
    "print(trainloader.dataset[0])\n",
    "\n",
    "print(f'shape of sample_img: {sample_img.shape}')\n",
    "print(f'shape fo sample_mask: {sample_mask.shape}')\n",
    "\n",
    "import torch.nn as nn\n",
    "class ModelSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the sam model to to fine-tune the model on a new dataset\n",
    "\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    freeze_encoder (bool): freeze the encoder weights\n",
    "    freeze_decoder (bool): freeze the decoder weights\n",
    "    freeze_prompt_encoder (bool): freeze the prompt encoder weights\n",
    "    transform (ResizeLongestSide): resize the images to the model input size\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    setup(): load the model and freeze the weights\n",
    "    forward(images, points): forward pass of the model, returns the masks and iou_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_encoder=True, freeze_decoder=False, freeze_prompt_encoder=True):\n",
    "        super().__init__()\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.freeze_decoder = freeze_decoder\n",
    "        self.freeze_prompt_encoder = freeze_prompt_encoder\n",
    "        # we need this to make the input image size compatible with the model\n",
    "        self.transform = ResizeLongestSide(1024) #This is 1024, because sam was trained on 1024x1024 images\n",
    "\n",
    "    def setup(self):\n",
    "        # TODO your path here\n",
    "        self.model = sam_model_registry['vit_l'](os.path.join('./sam_vit_l_0b3195.pth'))\n",
    "        # to speed up training time, we normally freeze the encoder and decoder\n",
    "        if self.freeze_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.transfrom = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "    def forward(self, images):\n",
    "        _, _, H, W = images.shape # batch, channel, height, width\n",
    "        image_embeddings = self.model.image_encoder(images) # shape: (1, 256, 64, 64)\n",
    "        # get prompt embeddings without acutally any prompts (uninformative)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        # get low resolution masks and iou predictions\n",
    "        # mulitmask_output=False means that we only get one mask per image,\n",
    "        # otherwise we would get three masks per image\n",
    "        low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # sparse_embeddings shape: (1, 0, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # dense_embeddings shape: (1, 256, 256)\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        # postprocess the masks to get the final masks and resize them to the original image size\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks, # shape: (1, 1, 256, 256)\n",
    "            (H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        # shape masks after interpolate: torch.Size([1, 1, 1024, 1024])\n",
    "        return masks, iou_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Repo\\\\SEM-SEG\\\\model\\\\SAM'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
