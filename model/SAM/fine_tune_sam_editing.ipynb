{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ndCIgtebCQO"
   },
   "source": [
    "# Notebook to fine-tune Segment Anything (SAM)\n",
    "This notebook goes together with this [medium post](https://medium.com/p/34514ee811bb/edit). The article provides a high-level overview of the concepts of how to fine-tune SAM. Read it first (if you haven't already), it will help to understand the colab. The structure will be:\n",
    "- 00 Installation and define Variables\n",
    "- 01 SAM without fine-tuning\n",
    "- 02 Define functions and classes for fine-tuning\n",
    "- 03 Plan for fine-tuning\n",
    "- 04 Define models and classes for fine-tuning\n",
    "- 05 Wrap up\n",
    "\n",
    "\n",
    "\n",
    "## 00 Installing packages and defining variables\n",
    "Start with the boring part. The code in this section will import everything you need and define the global variables. If you use your data, we also define this here, but I'll also provide a small dataset. So in summary:\n",
    "- install packages\n",
    "- define variables\n",
    "- adjust for your data\n",
    "\n",
    "Let's go!\n",
    "\n",
    "## Acknowledgments:\n",
    "\n",
    "I thank:\n",
    "- [Luca Medeiros](https://github.com/luca-medeiros/lightning-sam/tree/main/lightning_sam) whos code (even I could not get to run it on our cluster) helped me immensly in understanding SAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzlnytrWlHS5"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 21840,
     "status": "ok",
     "timestamp": 1722501064514,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "R2mNb494bCQS"
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrvbU3KFlLil"
   },
   "source": [
    "### Define variables\n",
    "In this section, we'll define global variables. Whenever I put TODO as a comment, you might want to change my path to your own data, or adjust model parameters, etc.\n",
    "\n",
    "To use data on Google Drive, we first need to mount Google Drive like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1722501064515,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "4XI7wl4amlQx",
    "outputId": "e408af0d-0b50-4b2b-a531-3fe3cf6cd759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Repo\\SEM-SEG\\model\\SAM\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1722501066315,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "Cm2M1Pqxl8nv",
    "outputId": "bcb74bf8-34e2-4ba9-c0b1-8d22e2938bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "global TRAIN\n",
    "global TEST\n",
    "global ANNOT\n",
    "\n",
    "# TODO: Put your path here !!!!\n",
    "TRAIN = os.path.join(\"./Dataset/Train\")\n",
    "TEST = os.path.join(\"./Dataset/Test\")\n",
    "ANNOT = \"annotations.json\"\n",
    "\n",
    "#model\n",
    "global BATCH_SIZE\n",
    "global EPOCHS\n",
    "global LR\n",
    "global WEIGHT_DECAY\n",
    "global DEVICE\n",
    "\n",
    "# TODO: adjust, if needed!!!!\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# loss\n",
    "global ALPHA\n",
    "global GAMMA\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "EVALUATE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhpwnMj986d"
   },
   "source": [
    "### Take a first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwOBqml4v_l6"
   },
   "source": [
    "When you use the web app of SAM, you might notice, that you need to provide a prompt (i.e. point with your mouse where your object is) to get a result. The mask_generator does this for you, by providing a grid of points over the whole image and creating a mask for each point and then later removing duplicated and low-quality masks. See the point grid below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seKWpfOqE9-E"
   },
   "source": [
    "## 02 Define models and classes for fine-tuning\n",
    "To make our life more easy, we'll define some functions and classes here:\n",
    "- Class COCODataset: helper to read in the annotations.json in coco format\n",
    "- class ResizeAndPad: helper to preprocess images to fit the expected size from SAM\n",
    "- load_datasets: function to get annotated COCO images into PyTorch dataloader\n",
    "\n",
    "You don't need to change anything here, you can just use the classes and functions later on.\n",
    "Feel free to skim through the code to gain better comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1722501114748,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "LAeC2TPHbCQU"
   },
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load data from a json file in COCO format.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        the root directory containing the images and annotations\n",
    "    annotation_file : str\n",
    "        name of the json file containing the annotations (in root_dir)\n",
    "    transform : callable\n",
    "        a function/transform to apply to each image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx)\n",
    "        returns the image, image path, and masks for the given index\n",
    "    buid_total_mask(masks)\n",
    "        combines the masks into a single mask\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        # Filter out image_ids without any annotations\n",
    "        self.image_ids = [image_id for image_id in self.image_ids if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return image, image_path, torch.tensor(masks).float()\n",
    "\n",
    "    def get_totalmask(self, masks):\n",
    "        \"\"\"get all masks in to one image\n",
    "        ARGS:\n",
    "            masks (List[Tensor]): list of masks\n",
    "        RETURNS:\n",
    "            total_gt (Tensor): all masks in one image\n",
    "\n",
    "        \"\"\"\n",
    "        total_gt = torch.zeros_like(masks[0][0,:,:])\n",
    "        for k in range(len(masks[0])):\n",
    "            total_gt += masks[0][k,:,:]\n",
    "        return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class ResizeAndPad:\n",
    "    \"\"\"\n",
    "    Resize and pad images and masks to a target size.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_size : int\n",
    "        the target size of the image\n",
    "    transform : ResizeLongestSide\n",
    "        a transform to resize the image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # Resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "        image = self.transform.apply_image(image)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "\n",
    "        # Pad image and masks to form a square\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "\n",
    "        return image, masks, bboxes\n",
    "\n",
    "\n",
    "def load_datasets(img_size):\n",
    "    \"\"\" load the training and validation datasets in PyTorch DataLoader objects\n",
    "    ARGS:\n",
    "        img_size (Tuple(int, int)): image size\n",
    "    RETURNS:\n",
    "        train_dataloader (DataLoader): training dataset\n",
    "        val_dataloader (DataLoader): validation dataset\n",
    "\n",
    "    \"\"\"\n",
    "    transform = ResizeAndPad(1024)\n",
    "    traindata = COCODataset(root_dir=TRAIN,\n",
    "                        annotation_file=os.path.join(TRAIN, ANNOT),\n",
    "                        transform=transform)\n",
    "    valdata = COCODataset(root_dir=TEST,\n",
    "                      annotation_file=os.path.join(TEST, ANNOT),\n",
    "                      transform=transform)\n",
    "    train_dataloader = DataLoader(traindata,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=1)\n",
    "    val_dataloader = DataLoader(valdata,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1647,
     "status": "ok",
     "timestamp": 1722501116387,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "C8syP6e2bCQW",
    "outputId": "224410e5-0d11-49d3-ee25-2214b45c0c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader = load_datasets(1024)\n",
    "sample_img = trainloader.dataset[0][0]\n",
    "sample_mask = trainloader.dataset[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1722501116388,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "KwxmF_hpbCQX",
    "outputId": "10dad886-3a85-4c78-bbca-6272603743ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sample_img: torch.Size([3, 1024, 1024])\n",
      "shape fo sample_mask: torch.Size([100, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of sample_img: {sample_img.shape}')\n",
    "print(f'shape fo sample_mask: {sample_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i1znVjo0KQg"
   },
   "source": [
    "### Summary functions and classes:\n",
    "Now we have transformed our images and coco annotations to torch tensors, that we can use for training. For training (fine-tuning SAM) we need to define a Neural net with PyTorch first, we do this in the next class. It's pretty well documented, so I'll leave you with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1722501116388,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "46NcQC-A3jfy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ModelSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the sam model to to fine-tune the model on a new dataset\n",
    "\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    freeze_encoder (bool): freeze the encoder weights\n",
    "    freeze_decoder (bool): freeze the decoder weights\n",
    "    freeze_prompt_encoder (bool): freeze the prompt encoder weights\n",
    "    transform (ResizeLongestSide): resize the images to the model input size\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    setup(): load the model and freeze the weights\n",
    "    forward(images, points): forward pass of the model, returns the masks and iou_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_encoder=True, freeze_decoder=False, freeze_prompt_encoder=True):\n",
    "        super().__init__()\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.freeze_decoder = freeze_decoder\n",
    "        self.freeze_prompt_encoder = freeze_prompt_encoder\n",
    "        # we need this to make the input image size compatible with the model\n",
    "        self.transform = ResizeLongestSide(1024) #This is 1024, because sam was trained on 1024x1024 images\n",
    "\n",
    "    def setup(self):\n",
    "        # TODO your path here\n",
    "        self.model = sam_model_registry['vit_h'](os.path.join('/content/drive/MyDrive/ColabNotebooks/finetunesam','sam_vit_h_4b8939.pth'))\n",
    "        # to speed up training time, we normally freeze the encoder and decoder\n",
    "        if self.freeze_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.transfrom = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "    def forward(self, images):\n",
    "        _, _, H, W = images.shape # batch, channel, height, width\n",
    "        image_embeddings = self.model.image_encoder(images) # shape: (1, 256, 64, 64)\n",
    "        # get prompt embeddings without acutally any prompts (uninformative)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        # get low resolution masks and iou predictions\n",
    "        # mulitmask_output=False means that we only get one mask per image,\n",
    "        # otherwise we would get three masks per image\n",
    "        low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # sparse_embeddings shape: (1, 0, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # dense_embeddings shape: (1, 256, 256)\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        # postprocess the masks to get the final masks and resize them to the original image size\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks, # shape: (1, 1, 256, 256)\n",
    "            (H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        # shape masks after interpolate: torch.Size([1, 1, 1024, 1024])\n",
    "        return masks, iou_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16442,
     "status": "ok",
     "timestamp": 1722501132826,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "V2fwU8qI4oKH",
    "outputId": "470649d2-6bb4-40a0-c357-cb8f0df21201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "model = ModelSimple()\n",
    "model.setup()\n",
    "img_size = model.model.image_encoder.img_size\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxu5a8633RQ-"
   },
   "source": [
    "## 03 Plan for fine-tuning\n",
    "When you've read the code in the cell above, this should be clear. We'll use the SAM model directly to output a 1024,1024 mask. This will be a probability map of the location of masks. As you see in line 45 above we don't provide any prompts. We do this to save us work with the postprocessing. To get all our masks for the image, we'll just put our ground truth masks (all masks for one image) in the loss function. So that we'll force the model to output high values for areas with masks.\n",
    "\n",
    "Now alt that's left is defining the loss functions and the training loop. For the loss function, we use the same as the authors of the SAM paper:\n",
    "\n",
    "Quote from Sam Paper\n",
    "```\n",
    "Losses. We supervise mask prediction with a linear combination of focal loss [65] and dice loss [73] in a 20:1 ratio of\n",
    "focal loss to dice loss, following [20, 14]. Unlike [20, 14],\n",
    "we observe that auxiliary deep supervision after each decoder layer is unhelpful. The IoU prediction head is trained\n",
    "with mean-square-error loss between the IoU prediction and\n",
    "the predicted mask's IoU with the ground truth mask. It is\n",
    "added to the mask loss with a constant scaling factor of 1.0.\n",
    "```\n",
    "We won't train the IoU prediction head in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPhQN81W3QyN"
   },
   "source": [
    "## 04 Models, classes functions for fine-tuning\n",
    "To make our code more clear, we'll define the following classes and functions to fine-tune SAM:\n",
    "- get_totalmask (function): combines all single object masks into one mask per image\n",
    "- FocalLoss (class): to calculate Focalloss\n",
    "- DiceLoss (class): to calculate Diceloss\n",
    "- criterion (function): combines Dice- and Focalloss. This is our final loss function\n",
    "- train_one_epoch(function): calculates loss for each batch\n",
    "- train (function) calls train_one_epoch for each epoch and calculates validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Rji2T7bCQa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1722501132826,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "5GhzOeOFbCQa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def get_totalmask(masks):\n",
    "    \"\"\"get all masks in to one image\n",
    "    ARGS:\n",
    "        masks (torch.Tensor): shape: (N, H, W) where N is the number of masks\n",
    "                              masks H,W is usually 1024,1024\n",
    "    RETURNS:\n",
    "        total_gt (torch.Tensor): all masks in one image\n",
    "\n",
    "    \"\"\"\n",
    "    total_gt = torch.zeros_like(masks[0,:,:])\n",
    "    for k in range(len(masks)):\n",
    "        total_gt += masks[k,:,:]\n",
    "    return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Computes the Focal loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = ALPHA * (1 - BCE_EXP)**GAMMA * BCE\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Computes the Dice loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / \\\n",
    "            (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "\n",
    "def criterion(x, y):\n",
    "    \"\"\" Combined dice and focal loss.\n",
    "    ARGS:\n",
    "        x: (torch.Tensor) the model output\n",
    "        y: (torch.Tensor) the target\n",
    "    RETURNS:\n",
    "        (torch.Tensor) the combined loss\n",
    "\n",
    "    \"\"\"\n",
    "    focal, dice = FocalLoss(), DiceLoss()\n",
    "    y = y.to(DEVICE)\n",
    "    x = x.to(DEVICE)\n",
    "    return 20 * focal(x, y) + dice(x, y)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, trainloader, optimizer, epoch_idx, tb_writer):\n",
    "    \"\"\" Runs forward and backward pass for one epoch and returns the average\n",
    "    batch loss for the epoch.\n",
    "    ARGS:\n",
    "        model: (nn.Module) the model to train\n",
    "        trainloader: (torch.utils.data.DataLoader) the dataloader for training\n",
    "        optimizer: (torch.optim.Optimizer) the optimizer to use for training\n",
    "        epoch_idx: (int) the index of the current epoch\n",
    "        tb_writer: (torch.utils.tensorboard.writer.SummaryWriter) the tensorboard writer\n",
    "    RETURNS:\n",
    "        last_loss: (float) the average batch loss for the epoch\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    for i, (image, path, masks) in enumerate(trainloader):\n",
    "\n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(image)\n",
    "        masks = masks[0].to(DEVICE)\n",
    "        total_mask = get_totalmask(masks)\n",
    "        pred = pred.to(DEVICE)\n",
    "        loss = criterion(pred, total_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    i = len(trainloader)\n",
    "    last_loss = running_loss / i\n",
    "    print(f'batch_loss for batch {i}: {last_loss}')\n",
    "    tb_x = epoch_idx * len(trainloader) + i + 1\n",
    "    tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def train_fn():\n",
    "    \"\"\" Trains the model for the given number of EPOCHS.\"\"\"\n",
    "    bestmodel_path = \"\"\n",
    "    model = ModelSimple()\n",
    "    model.setup()\n",
    "    model.to(DEVICE)\n",
    "    img_size = model.model.image_encoder.img_size\n",
    "    trainloader, validloader = load_datasets(img_size=img_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    best_valid_loss = float('inf')\n",
    "    timestamp_writer = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(os.path.join(\"/content/drive/MyDrive/ColabNotebooks/finetunesam\", f\"trainer_{timestamp_writer}\")) # TODO your path here\n",
    "    for epch in range(EPOCHS): # type: ignore\n",
    "        running_vloss = 0.\n",
    "        model.train(True)\n",
    "        avg_batchloss = train_one_epoch(\n",
    "            model, trainloader, optimizer, epch, writer)\n",
    "        if not EVALUATE: # type: ignore\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            for images, path, masks in validloader:\n",
    "                model.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks[0].to(DEVICE)\n",
    "                total_mask = get_totalmask(masks)\n",
    "                total_mask = total_mask.to(DEVICE)\n",
    "                model.eval()\n",
    "                preds, iou = model(images)\n",
    "                preds = preds.to(DEVICE)\n",
    "                vloss = criterion(preds, total_mask)\n",
    "                running_vloss += vloss.item()\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        avg_vloss = running_vloss / len(validloader)\n",
    "        # save model\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        print(f'best valid loss: {best_valid_loss}')\n",
    "        if running_vloss < best_valid_loss:\n",
    "          best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFWOAdEY7upr"
   },
   "source": [
    "## 05 Wrap up and next steps\n",
    "\n",
    "Training the model on Colab might run out of RAM, when you don't have premium (at least it did for me) I've trained the model on an HPC cluster. I'll leave you with the code to train the model and also some functions to visualize and evaluate.\n",
    "\n",
    "### Train:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSqrkj4w7uLh",
    "outputId": "d00af78c-b9e6-401c-c1ef-94293ecb5c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "model = train_fn()\n",
    "#torch.save(model.state_dict(), os.path.join('/content/drive/MyDrive/ColabNotebooks/finetunesam','finetunet.pth')) # TODO your path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FrAGnJX8vwH"
   },
   "source": [
    "## Predict trained model\n",
    "\n",
    "Here I assume that you have fine-tuned SAM with the code above and the fine-tuned model is saved as model_final.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1722501156420,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "bf8QC8hobCQe"
   },
   "outputs": [],
   "source": [
    "# use my custom SAM model wrapper class\n",
    "#model_trained= ModelSimple()\n",
    "#model_trained.setup()\n",
    "#model_trained.load_state_dict(torch.load('finetuned.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1722501156420,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "lU1aS-ow9ZUy"
   },
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    " #   model_trained.load_state_dict(torch.load('finetuned.pth', map_location=torch.device('cpu')))\n",
    " #   model_trained.eval()\n",
    "  #  img_size = model_trained.model.image_encoder.img_size\n",
    "   # print(f'img_size: {img_size}')\n",
    "    #print(f'img_size: {model_trained.model.image_encoder.img_size}')\n",
    "    # TODO your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1722501156420,
     "user": {
      "displayName": "Maximilian Joas",
      "userId": "04914600760403428307"
     },
     "user_tz": -120
    },
    "id": "mwTOPNiemlQ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
